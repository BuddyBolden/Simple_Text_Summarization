<div class="step-text">
<h5 id="theory">Theory</h5>
<p>In this stage, you will try out the TF-IDF approach. It is a modified word probability approach used to calculate how important a word is for a document in a text collection. This approach is based on the two assumptions:</p>
<ol>
<li>Frequent words have more weight in the document.</li>
<li>The smaller the number of documents that contain the word, the more important the word is for the document that contains it.</li>
</ol>
<p>In this project, we need to calculate the TF-IDF for every word in <strong>all</strong> news stories. Note that for this project, we consider one sentence as a whole document.</p>
<h5 id="description">Description</h5>
<p>TF-IDF reflects the importance of a word for a specific document in a collection of documents. This measure was widely used by search engines to score and rank documents according to the user's query. It was also used for text classification and text summarization, two of the fundamental NLP tasks. This method is not computationally expensive and yields great results for text summarization tasks.</p>
<p>In this stage, you will get familiar with some theory behind this method and work with one of the most popular machine learning libraries for Python <code class="language-python">sklearn</code>.</p>
<p>In this stage, you may also find the function <code class="language-python">toarray()</code> useful. It converts a sparse matrix that <code class="language-python">sklearn</code> uses to an ordinary n-dimensional array, so, using indexation, you may access the terms and their scores in a more familiar way. Check out the <a href="https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/" rel="noopener noreferrer nofollow" target="_blank">Machine Learning Master tutorial</a> on how to encode the text data for machine learning.</p>
<h5 id="objectives">Objectives</h5>
<p>Steps 1-6 are the same as in the second stage. At this stage, you need to:</p>
<ol>
<li>Read an XML file with news texts.</li>
<li>Extract the headers and the news texts.</li>
<li>Process each news story: first, tokenize the text into separate sentences.</li>
<li>Find <em><em><span class="math-tex">\(\sqrt{N}\)</span></em></em>  (<em><em><em><em><span class="math-tex">\(N\)</span></em></em></em></em> is the number of the source text's sentences).</li>
<li>Tokenize sentences into words, convert the tokenized words to lowercase, and get rid of punctuation and stopwords.</li>
<li>Lemmatize each word via the <code class="language-python">WordNetLemmatizer</code>.</li>
<li>Pass the collection of preprocessed sentences to the <code class="language-python">TfidfVectorizer</code> (each standalone news story is as a separate dataset). Each sentence within one news story will represent a separate document (this way, the first story will comprise <code class="language-python">16</code> documents in total, the second news story â€” <code class="language-python">6</code> documents, and so on). Specify the <code class="language-python">word_tokenize()</code> in the <code class="language-python">tokenizer</code> parameter of the <code class="language-python">TfidfVectorizer</code>:
	<pre><code class="language-python">model = TfidfVectorizer(tokenizer=word_tokenize)</code></pre>
</li>
<li>Count the mean for each document in the resultant TF-IDF matrix (you can use <code class="language-python">numpy.mean()</code> here). To access the term weights of a particular document, use the indexation; only those values that are greater than zero should be calculated.</li>
<li>Find <em><span class="math-tex">\(\sqrt{N}\)</span></em> best scoring sentences.</li>
<li>Print the header and the selected sentences. Mind that you need to print the sentences in the order in which they appear in the original text.</li>
<li>Repeat Steps 3-10 for each news story.</li>
</ol>
<p>Your program does not get any input in this stage: you should just read the news from the <em>news.xml</em> file. Do not make any changes to this file. The number of news pieces in one file may vary. You may find examples of its structure below.</p>
<p>Once you got the summaries for each piece of news, print them as you did in the previous stages. The summarized news texts should consist of unprocessed sentences.</p>
<h5 id="example">Example</h5>
<p><strong>Example 1:</strong> <em>Input file structure</em></p>
<pre><code class="language-xml">&lt;?xml version='1.0' encoding='UTF8'?&gt;
&lt;data&gt;
  &lt;corpus&gt;
    &lt;news&gt;
      &lt;value name="head"&gt;New Portuguese skull may be an early relative of Neandertals&lt;/value&gt;
      &lt;value name="text"&gt;Half a million years ago, several different members of our genus, Homo, had spread throughout Europe and Asia, where some would eventually evolve into Neandertals. 
      But which ones has been the subject of intense debate. 
      A newly discovered partial skull is offering another clue to help solve the mystery of the ancestry of Neandertals. 
      Found in 2014 in the Gruta da Aroeira cave in central Portugal with ancient stone hand axes, the skull (3D reconstruction pictured) is firmly dated to 400,000 years old and an archaic member of our genus, according to a study published today in the Proceedings of the National Academy of Sciences. 
      The skull shows a new mix of features not seen before in fossil humans - it has traits that link it to Neandertals, such as a fused brow ridge, as well as some primitive traits that resemble other extinct fossils in Europe. 
      This new combination of features on a well-dated skull may help researchers sort out how different fossils in Europe are related to each other - and which ones eventually evolved into Neandertals.&lt;/value&gt;
    &lt;/news&gt;
    &lt;news&gt;
      &lt;value name="head"&gt;Loneliness May Make Quitting Smoking Even Tougher&lt;/value&gt;
      &lt;value name="text"&gt;Being lonely may make it harder to quit smoking, a new British study suggests.
      Using genetic and survey data from hundreds of thousands of people, researchers found that loneliness makes it more likely that someone will smoke.
      This type of analysis is called Mendelian randomization.
      ' This method has never been applied to this question before and so the results are novel, but also tentative,'  said co-lead author Robyn Wootton, a senior research associate at the University of Bristol in the United Kingdom.
      ' We found evidence to suggest that loneliness leads to increased smoking, with people more likely to start smoking, to smoke more cigarettes and to be less likely to quit,'  Wootton said in a university news release.
      These data mesh with an observation that during the coronavirus pandemic, more British people are smoking.
      Senior study author Jorien Treur said, ' Our finding that smoking may also lead to more loneliness is tentative, but it is in line with other recent studies that identified smoking as a risk factor for poor mental health.
      A potential mechanism for this relationship is that nicotine from cigarette smoke interferes with neurotransmitters such as dopamine in the brain.' 
      Treur is a visiting research associate from Amsterdam UMC.
      The researchers also looked for a connection between loneliness and drinking but found none.
      Still, if loneliness causes people to smoke, it is important to alert smoking cessation services so they can add this factor as they help people to quit, the study authors said.
      The report was published June 16 in the journal Addiction.&lt;/value&gt;
    &lt;/news&gt;
  &lt;/corpus&gt;
&lt;/data&gt;</code></pre>
<p><em>Output</em></p>
<pre><code class="language-no-highlight">HEADER: New Portuguese skull may be an early relative of Neandertals
TEXT: But which ones has been the subject of intense debate. 
A newly discovered partial skull is offering another clue to help solve the mystery of the ancestry of Neandertals.

HEADER: Loneliness May Make Quitting Smoking Even Tougher
TEXT: This type of analysis is called Mendelian randomization. 
Treur is a visiting research associate from Amsterdam UMC. 
The report was published June 16 in the journal Addiction.</code></pre>
</div>